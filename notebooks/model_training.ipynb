{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Classification Model Training\n",
    "\n",
    "This notebook demonstrates the training of exercise classification models using the extracted features. We'll train models for various exercise attributes including:\n",
    "\n",
    "- Target muscle groups\n",
    "- Required equipment\n",
    "- Movement patterns\n",
    "- Intensity/experience levels\n",
    "- Exercise types\n",
    "- Quality of movement\n",
    "- Risk assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set paths\n",
    "MODEL_DIR = \"models\"\n",
    "RESULTS_DIR = \"results\"\n",
    "DATA_DIR = \"data/processed\"\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data\n",
    "\n",
    "First, let's load the processed features and prepare them for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load processed features\n",
    "features_path = f\"{DATA_DIR}/combined_features.csv\"\n",
    "if os.path.exists(features_path):\n",
    "    features_df = pd.read_csv(features_path)\n",
    "    print(f\"Loaded {len(features_df)} exercises with {len(features_df.columns)} features\")\n",
    "else:\n",
    "    print(\"Processed features not found. Please run the feature engineering notebook first.\")\n",
    "    features_df = None\n",
    "\n",
    "# Define feature columns\n",
    "if features_df is not None:\n",
    "    feature_columns = {\n",
    "        'text': [col for col in features_df.columns if col.startswith(('tfidf_', 'text_pca_'))],\n",
    "        'pose': [col for col in features_df.columns if col.startswith(('landmark_', 'pose_pca_'))],\n",
    "        'targets': [\n",
    "            'bodypart', 'target', 'equipment', 'movement_pattern',\n",
    "            'intensity_level', 'exercise_type', 'movement_quality',\n",
    "            'risk_assessment'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    print(\"\\nFeature columns:\")\n",
    "    for key, cols in feature_columns.items():\n",
    "        print(f\"{key}: {len(cols)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Model Architecture\n",
    "\n",
    "Let's define our multimodal neural network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class ExerciseDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for exercise data\"\"\"\n",
    "    def __init__(self, text_features, pose_features, labels):\n",
    "        self.text_features = torch.tensor(text_features, dtype=torch.float32)\n",
    "        self.pose_features = torch.tensor(pose_features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.text_features[idx], self.pose_features[idx], self.labels[idx]\n",
    "\n",
    "class MultimodalExerciseClassifier(nn.Module):\n",
    "    \"\"\"Neural network for classifying exercises using multimodal features\"\"\"\n",
    "    def __init__(self, text_input_dim, pose_input_dim, num_classes, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Text processing branch\n",
    "        self.text_layers = nn.Sequential(\n",
    "            nn.Linear(text_input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        # Pose processing branch\n",
    "        self.pose_layers = nn.Sequential(\n",
    "            nn.Linear(pose_input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        # Combined layers\n",
    "        self.combined_layers = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, text_features, pose_features):\n",
    "        text_output = self.text_layers(text_features)\n",
    "        pose_output = self.pose_layers(pose_features)\n",
    "        combined = torch.cat((text_output, pose_output), dim=1)\n",
    "        return self.combined_layers(combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Training Functions\n",
    "\n",
    "Let's create the functions needed for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def prepare_data(df, target_col, text_cols, pose_cols, test_size=0.2, val_size=0.1):\n",
    "    \"\"\"Prepare data for training\"\"\"\n",
    "    # Extract features\n",
    "    text_features = df[text_cols].fillna(0).values if text_cols else np.zeros((len(df), 1))\n",
    "    pose_features = df[pose_cols].fillna(0).values if pose_cols else np.zeros((len(df), 1))\n",
    "    \n",
    "    # Encode target\n",
    "    encoder = LabelEncoder()\n",
    "    labels = encoder.fit_transform(df[target_col])\n",
    "    \n",
    "    # Split data\n",
    "    X_train_text, X_test_text, X_train_pose, X_test_pose, y_train, y_test = train_test_split(\n",
    "        text_features, pose_features, labels, test_size=test_size, random_state=42, stratify=labels\n",
    "    )\n",
    "    \n",
    "    X_train_text, X_val_text, X_train_pose, X_val_pose, y_train, y_val = train_test_split(\n",
    "        X_train_text, X_train_pose, y_train, test_size=val_size/(1-test_size), random_state=42, stratify=y_train\n",
    "    )\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = ExerciseDataset(X_train_text, X_train_pose, y_train)\n",
    "    val_dataset = ExerciseDataset(X_val_text, X_val_pose, y_val)\n",
    "    test_dataset = ExerciseDataset(X_test_text, X_test_pose, y_test)\n",
    "    \n",
    "    return {\n",
    "        'train_dataset': train_dataset,\n",
    "        'val_dataset': val_dataset,\n",
    "        'test_dataset': test_dataset,\n",
    "        'encoder': encoder\n",
    "    }\n",
    "\n",
    "def train_model(data, target_col, batch_size=32, learning_rate=0.001, epochs=30, patience=5):\n",
    "    \"\"\"Train a model for a specific target\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(data['train_dataset'], batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(data['val_dataset'], batch_size=batch_size)\n",
    "    \n",
    "    # Create model\n",
    "    text_dim = data['train_dataset'].text_features.shape[1]\n",
    "    pose_dim = data['train_dataset'].pose_features.shape[1]\n",
    "    num_classes = len(data['encoder'].classes_)\n",
    "    \n",
    "    model = MultimodalExerciseClassifier(text_dim, pose_dim, num_classes).to(device)\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for text, pose, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs} (Train)'):\n",
    "            text, pose, labels = text.to(device), pose.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(text, pose)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        train_acc = 100 * train_correct / train_total\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for text, pose, labels in tqdm(val_loader, desc=f'Epoch {epoch+1}/{epochs} (Val)'):\n",
    "                text, pose, labels = text.to(device), pose.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = model(text, pose)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        \n",
    "        # Update history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}:')\n",
    "        print(f'  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "        print(f'  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            # Save best model\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'history': history,\n",
    "                'target_col': target_col,\n",
    "                'feature_names': {\n",
    "                    'text': text_cols,\n",
    "                    'pose': pose_cols\n",
    "                }\n",
    "            }, f\"{MODEL_DIR}/{target_col}_model.pth\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping triggered after {epoch+1} epochs')\n",
    "                break\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'history': history,\n",
    "        'target_col': target_col\n",
    "    }\n",
    "\n",
    "def evaluate_model(model, test_dataset, encoder, target_col):\n",
    "    \"\"\"Evaluate a trained model\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text, pose, labels in test_loader:\n",
    "            text, pose = text.to(device), pose.to(device)\n",
    "            outputs = model(text, pose)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    report = classification_report(all_labels, all_preds, target_names=encoder.classes_)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'classification_report': report\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Models\n",
    "\n",
    "Now, let's train models for each target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Select target variables to train models for\n",
    "if features_df is not None and 'feature_columns' in locals():\n",
    "    # You can select specific targets here or use all available ones\n",
    "    target_variables = feature_columns['targets']\n",
    "    \n",
    "    print(f\"Training models for {len(target_variables)} target variables:\")\n",
    "    for target in target_variables:\n",
    "        print(f\"  - {target}\")\n",
    "    \n",
    "    # Ask for confirmation\n",
    "    confirm = input(\"\\nProceed with training? (y/n): \")\n",
    "    \n",
    "    if confirm.lower() == 'y':\n",
    "        # Dictionary to store training results\n",
    "        training_results = {}\n",
    "        \n",
    "        # Train a model for each target variable\n",
    "        for target in target_variables:\n",
    "            print(f\"\\n{'='*80}\\nTraining model for {target}\\n{'='*80}\")\n",
    "            \n",
    "            # Prepare data for this target\n",
    "            data = prepare_data(\n",
    "                features_df, \n",
    "                target, \n",
    "                feature_columns['text'], \n",
    "                feature_columns['pose'],\n",
    "                test_size=0.2,\n",
    "                val_size=0.1\n",
    "            )\n",
    "            \n",
    "            # Train model\n",
    "            result = train_model(\n",
    "                data,\n",
    "                target,\n",
    "                batch_size=32,\n",
    "                learning_rate=0.001,\n",
    "                epochs=30,\n",
    "                patience=5\n",
    "            )\n",
    "            \n",
    "            # Evaluate model\n",
    "            print(f\"\\nEvaluating model for {target}\")\n",
    "            eval_result = evaluate_model(\n",
    "                result['model'],\n",
    "                data['test_dataset'],\n",
    "                data['encoder'],\n",
    "                target\n",
    "            )\n",
    "            \n",
    "            # Store results\n",
    "            training_results[target] = {\n",
    "                \"training\": result,\n",
    "                \"evaluation\": eval_result\n",
    "            }\n",
    "        \n",
    "        print(\"\\nTraining complete! Results summary:\")\n",
    "        for target, result in training_results.items():\n",
    "            print(f\"\\n{target}:\")\n",
    "            print(f\"  Accuracy: {result['evaluation']['accuracy']:.4f}\")\n",
    "            if 'classification_report' in result['evaluation']:\n",
    "                report = result['evaluation']['classification_report']\n",
    "                if 'weighted avg' in report:\n",
    "                    print(f\"  Precision: {report['weighted avg']['precision']:.4f}\")\n",
    "                    print(f\"  Recall: {report['weighted avg']['recall']:.4f}\")\n",
    "                    print(f\"  F1 Score: {report['weighted avg']['f1-score']:.4f}\")\n",
    "    else:\n",
    "        print(\"Training cancelled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Model Results\n",
    "\n",
    "Let's visualize the results of our trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize model results\n",
    "if 'training_results' in locals() and training_results:\n",
    "    # Create figure for accuracy comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Extract accuracies\n",
    "    targets = list(training_results.keys())\n",
    "    accuracies = [result['evaluation']['accuracy'] for result in training_results.values()]\n",
    "    \n",
    "    # Sort by accuracy\n",
    "    sorted_indices = np.argsort(accuracies)\n",
    "    sorted_targets = [targets[i] for i in sorted_indices]\n",
    "    sorted_accuracies = [accuracies[i] for i in sorted_indices]\n",
    "    \n",
    "    # Plot accuracies\n",
    "    ax = sns.barplot(x=sorted_accuracies, y=sorted_targets)\n",
    "    \n",
    "    # Add accuracy values\n",
    "    for i, acc in enumerate(sorted_accuracies):\n",
    "        ax.text(acc + 0.02, i, f\"{acc:.4f}\", va='center')\n",
    "    \n",
    "    plt.title(\"Model Accuracy Comparison\")\n",
    "    plt.xlabel(\"Accuracy\")\n",
    "    plt.ylabel(\"Target Variable\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{RESULTS_DIR}/model_accuracy_comparison.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Make Predictions\n",
    "\n",
    "Let's show how to use the trained models to make predictions on new exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_model(target_col, model_dir=MODEL_DIR):\n",
    "    \"\"\"Load a trained model\"\"\"\n",
    "    model_path = f\"{model_dir}/{target_col}_model.pth\"\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Model for {target_col} not found\")\n",
    "        return None\n",
    "    \n",
    "    checkpoint = torch.load(model_path)\n",
    "    \n",
    "    # Create model with same architecture\n",
    "    text_dim = len(checkpoint['feature_names']['text'])\n",
    "    pose_dim = len(checkpoint['feature_names']['pose'])\n",
    "    num_classes = len(checkpoint['target_col'].split(','))\n",
    "    \n",
    "    model = MultimodalExerciseClassifier(text_dim, pose_dim, num_classes)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'metadata': checkpoint\n",
    "    }\n",
    "\n",
    "def predict(model, text_features, pose_features, text_scaler, pose_scaler, encoder):\n",
    "    \"\"\"Make predictions using a trained model\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Scale features\n",
    "    text_features = text_scaler.transform(text_features)\n",
    "    pose_features = pose_scaler.transform(pose_features)\n",
    "    \n",
    "    # Convert to tensors\n",
    "    text_tensor = torch.tensor(text_features, dtype=torch.float32).to(device)\n",
    "    pose_tensor = torch.tensor(pose_features, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(text_tensor, pose_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    # Convert to numpy and decode\n",
    "    predicted = predicted.cpu().numpy()\n",
    "    predicted = encoder.inverse_transform(predicted)\n",
    "    \n",
    "    return predicted\n",
    "\n",
    "# Example: Load a model and make predictions\n",
    "if 'feature_columns' in locals():\n",
    "    target_to_predict = feature_columns['targets'][0]  # Use first target as example\n",
    "    \n",
    "    print(f\"Loading model for {target_to_predict}\")\n",
    "    loaded_model = load_model(target_to_predict)\n",
    "    \n",
    "    if loaded_model:\n",
    "        print(f\"Model loaded successfully\")\n",
    "        print(f\"Model metadata: {loaded_model['metadata']}\")\n",
    "        \n",
    "        # Select some examples to predict\n",
    "        num_examples = 5\n",
    "        examples = features_df.sample(num_examples)\n",
    "        \n",
    "        # Extract features\n",
    "        text_features = examples[feature_columns['text']].fillna(0).values\n",
    "        pose_features = examples[feature_columns['pose']].fillna(0).values\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = predict(\n",
    "            loaded_model['model'],\n",
    "            text_features,\n",
    "            pose_features,\n",
    "            loaded_model['metadata']['text_scaler'],\n",
    "            loaded_model['metadata']['pose_scaler'],\n",
    "            loaded_model['metadata']['encoder']\n",
    "        )\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\nPredictions for {target_to_predict}:\")\n",
    "        for i, (_, example) in enumerate(examples.iterrows()):\n",
    "            print(f\"\\nExample {i+1}:\")\n",
    "            print(f\"  Name: {example.get('name', 'Unknown')}\")\n",
    "            print(f\"  Body Part: {example.get('bodypart', 'Unknown')}\")\n",
    "            print(f\"  Actual {target_to_predict}: {example.get(target_to_predict, 'Unknown')}\")\n",
    "            print(f\"  Predicted {target_to_predict}: {predictions[i]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}