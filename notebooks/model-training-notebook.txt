{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Classification Model Training\n",
    "\n",
    "This notebook demonstrates how to train deep learning models for exercise classification. We'll use the features extracted in the feature engineering phase to train models that can classify exercises based on:\n",
    "\n",
    "- Target muscle groups\n",
    "- Movement patterns\n",
    "- Exercise type (compound vs. isolation, strength vs. hypertrophy)\n",
    "- Intensity/experience level\n",
    "- Risk assessment\n",
    "- Movement quality\n",
    "\n",
    "We'll use PyTorch to create multimodal models that combine text and pose features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import joblib\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Handle path for importing custom modules\n",
    "# Add the parent directory to the path to import from src\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set Matplotlib and Seaborn styles\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set_palette('Set2')\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Processed Data\n",
    "\n",
    "First, let's load the processed data with features extracted in the feature engineering phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set data paths\n",
    "PROCESSED_DATA_DIR = \"data/processed\"\n",
    "MODEL_DIR = \"models\"\n",
    "RESULTS_DIR = \"results\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# Load all features data\n",
    "features_path = f\"{PROCESSED_DATA_DIR}/all_features.csv\"\n",
    "\n",
    "if os.path.exists(features_path):\n",
    "    features_df = pd.read_csv(features_path)\n",
    "    print(f\"Loaded {len(features_df)} exercises with {len(features_df.columns)} features\")\n",
    "else:\n",
    "    print(\"All features data not found. Please run the feature engineering notebook first.\")\n",
    "    features_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Display basic info and sample data\n",
    "if features_df is not None:\n",
    "    print(\"\\nDataFrame Info:\")\n",
    "    features_df.info()\n",
    "    \n",
    "    print(\"\\nSample Data:\")\n",
    "    display(features_df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Model Architecture\n",
    "\n",
    "Now, let's define the neural network architecture for our multimodal exercise classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class MultimodalExerciseClassifier(nn.Module):\n",
    "    \"\"\"Neural network for classifying exercises using multimodal features\"\"\"\n",
    "    def __init__(self, \n",
    "                 text_input_dim, \n",
    "                 pose_input_dim,\n",
    "                 num_classes,\n",
    "                 hidden_dim=128):\n",
    "        \"\"\"Initialize model\n",
    "        \n",
    "        Args:\n",
    "            text_input_dim: Dimension of text features\n",
    "            pose_input_dim: Dimension of pose features\n",
    "            num_classes: Number of output classes\n",
    "            hidden_dim: Hidden dimension size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Text processing branch\n",
    "        self.text_layers = nn.Sequential(\n",
    "            nn.Linear(text_input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        # Pose processing branch\n",
    "        self.pose_layers = nn.Sequential(\n",
    "            nn.Linear(pose_input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        # Combined layers\n",
    "        self.combined_layers = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, text_features, pose_features):\n",
    "        \"\"\"Forward pass\n",
    "        \n",
    "        Args:\n",
    "            text_features: Text features\n",
    "            pose_features: Pose features\n",
    "            \n",
    "        Returns:\n",
    "            Output logits\n",
    "        \"\"\"\n",
    "        text_output = self.text_layers(text_features)\n",
    "        pose_output = self.pose_layers(pose_features)\n",
    "        \n",
    "        # Concatenate the outputs\n",
    "        combined = torch.cat((text_output, pose_output), dim=1)\n",
    "        \n",
    "        # Pass through combined layers\n",
    "        output = self.combined_layers(combined)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class ExerciseDataset(Dataset):\n",
    "    \"\"\"Dataset for exercise classification\"\"\"\n",
    "    def __init__(self, text_features, pose_features, labels):\n",
    "        \"\"\"Initialize dataset\n",
    "        \n",
    "        Args:\n",
    "            text_features: Text features matrix\n",
    "            pose_features: Pose features matrix\n",
    "            labels: Labels vector\n",
    "        \"\"\"\n",
    "        self.text_features = torch.tensor(text_features, dtype=torch.float32)\n",
    "        self.pose_features = torch.tensor(pose_features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.text_features[idx], self.pose_features[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation Helper Functions\n",
    "\n",
    "Let's define helper functions for preparing the data for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def prepare_data(df, target_col, text_feature_cols, pose_feature_cols, test_size=0.2, val_size=0.1, random_state=42):\n",
    "    \"\"\"Prepare data for model training\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        target_col: Target column name\n",
    "        text_feature_cols: List of text feature columns\n",
    "        pose_feature_cols: List of pose feature columns\n",
    "        test_size: Test set size ratio\n",
    "        val_size: Validation set size ratio\n",
    "        random_state: Random seed\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with prepared data and metadata\n",
    "    \"\"\"\n",
    "    # Check if target column exists\n",
    "    if target_col not in df.columns:\n",
    "        raise ValueError(f\"Target column '{target_col}' not found in DataFrame\")\n",
    "    \n",
    "    # Encode target variable\n",
    "    encoder = LabelEncoder()\n",
    "    y = encoder.fit_transform(df[target_col])\n",
    "    \n",
    "    # Get number of classes\n",
    "    num_classes = len(encoder.classes_)\n",
    "    print(f\"Target: {target_col} with {num_classes} classes: {encoder.classes_}\")\n",
    "    \n",
    "    # Extract features\n",
    "    if not text_feature_cols:\n",
    "        # Use dummy feature if no text features provided\n",
    "        X_text = np.zeros((len(df), 1))\n",
    "        print(\"No text features provided. Using dummy features.\")\n",
    "    else:\n",
    "        X_text = df[text_feature_cols].fillna(0).values\n",
    "        print(f\"Using {len(text_feature_cols)} text features\")\n",
    "        \n",
    "    if not pose_feature_cols:\n",
    "        # Use dummy feature if no pose features provided\n",
    "        X_pose = np.zeros((len(df), 1))\n",
    "        print(\"No pose features provided. Using dummy features.\")\n",
    "    else:\n",
    "        X_pose = df[pose_feature_cols].fillna(0).values\n",
    "        print(f\"Using {len(pose_feature_cols)} pose features\")\n",
    "    \n",
    "    # Scale features\n",
    "    text_scaler = StandardScaler()\n",
    "    X_text_scaled = text_scaler.fit_transform(X_text)\n",
    "    \n",
    "    pose_scaler = StandardScaler()\n",
    "    X_pose_scaled = pose_scaler.fit_transform(X_pose)\n",
    "    \n",
    "    # Initial train/test split\n",
    "    X_text_train, X_text_test, X_pose_train, X_pose_test, y_train, y_test = train_test_split(\n",
    "        X_text_scaled, X_pose_scaled, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Further split training set into train/validation\n",
    "    if val_size > 0:\n",
    "        # Calculate validation size relative to training set\n",
    "        rel_val_size = val_size / (1 - test_size)\n",
    "        \n",
    "        X_text_train, X_text_val, X_pose_train, X_pose_val, y_train, y_val = train_test_split(\n",
    "            X_text_train, X_pose_train, y_train, \n",
    "            test_size=rel_val_size, random_state=random_state, stratify=y_train\n",
    "        )\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = ExerciseDataset(X_text_train, X_pose_train, y_train)\n",
    "        val_dataset = ExerciseDataset(X_text_val, X_pose_val, y_val)\n",
    "        test_dataset = ExerciseDataset(X_text_test, X_pose_test, y_test)\n",
    "        \n",
    "        print(f\"Data split: {len(y_train)} train, {len(y_val)} validation, {len(y_test)} test samples\")\n",
    "    else:\n",
    "        # No validation set\n",
    "        train_dataset = ExerciseDataset(X_text_train, X_pose_train, y_train)\n",
    "        val_dataset = None\n",
    "        test_dataset = ExerciseDataset(X_text_test, X_pose_test, y_test)\n",
    "        \n",
    "        print(f\"Data split: {len(y_train)} train, {len(y_test)} test samples\")\n",
    "    \n",
    "    # Calculate class weights for imbalanced datasets\n",
    "    class_counts = np.bincount(y_train)\n",
    "    class_weights = 1.0 / class_counts\n",
    "    class_weights = class_weights / class_weights.sum() * len(class_weights)  # Normalize\n",
    "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32)\n",
    "    \n",
    "    # Check class imbalance\n",
    "    print(\"\\nClass distribution in training set:\")\n",
    "    for i, count in enumerate(class_counts):\n",
    "        percentage = count / len(y_train) * 100\n",
    "        class_name = encoder.inverse_transform([i])[0]\n",
    "        print(f\"  {class_name}: {count} samples ({percentage:.1f}%), weight: {class_weights[i]:.2f}\")\n",
    "    \n",
    "    # Return prepared data\n",
    "    return {\n",
    "        \"train_dataset\": train_dataset,\n",
    "        \"val_dataset\": val_dataset,\n",
    "        \"test_dataset\": test_dataset,\n",
    "        \"class_weights\": class_weights_tensor,\n",
    "        \"num_classes\": num_classes,\n",
    "        \"encoder\": encoder,\n",
    "        \"text_scaler\": text_scaler,\n",
    "        \"pose_scaler\": pose_scaler,\n",
    "        \"text_input_dim\": X_text.shape[1],\n",
    "        \"pose_input_dim\": X_pose.shape[1],\n",
    "        \"feature_names\": {\n",
    "            \"text\": text_feature_cols,\n",
    "            \"pose\": pose_feature_cols\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def extract_feature_columns(df):\n",
    "    \"\"\"Extract feature column lists by type\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with feature column lists\n",
    "    \"\"\"\n",
    "    # Text features\n",
    "    text_feature_cols = [col for col in df.columns if col.startswith(('tfidf_', 'text_pca_', 'kw_'))]\n",
    "    \n",
    "    # Pose features\n",
    "    pose_feature_cols = [col for col in df.columns if col.startswith(('landmark_', 'pose_pca_', 'pose_'))]\n",
    "    \n",
    "    # Combined features\n",
    "    combined_feature_cols = text_feature_cols + pose_feature_cols\n",
    "    \n",
    "    # Target columns\n",
    "    target_cols = [col for col in df.columns if col in [\n",
    "        'movement_pattern', 'is_compound', 'exercise_type', 'intensity_level', \n",
    "        'risk_assessment', 'combined_risk_assessment', 'movement_quality'\n",
    "    ]]\n",
    "    \n",
    "    return {\n",
    "        \"text\": text_feature_cols,\n",
    "        \"pose\": pose_feature_cols,\n",
    "        \"combined\": combined_feature_cols,\n",
    "        \"targets\": target_cols\n",
    "    }\n",
    "\n",
    "# Extract feature columns\n",
    "if features_df is not None:\n",
    "    feature_columns = extract_feature_columns(features_df)\n",
    "    \n",
    "    print(\"Available features:\")\n",
    "    print(f\"  Text features: {len(feature_columns['text'])}\")\n",
    "    print(f\"  Pose features: {len(feature_columns['pose'])}\")\n",
    "    print(f\"  Combined features: {len(feature_columns['combined'])}\")\n",
    "    \n",
    "    print(\"\\nAvailable target variables:\")\n",
    "    for target in feature_columns['targets']:\n",
    "        if features_df[target].dtype == bool:\n",
    "            num_classes = 2\n",
    "        else:\n",
    "            num_classes = features_df[target].nunique()\n",
    "        print(f\"  {target}: {num_classes} classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Function\n",
    "\n",
    "Now, let's define a function to train a model for a specific target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def train_model(data, target_col, batch_size=32, learning_rate=0.001, weight_decay=1e-5, \n",
    "                epochs=50, patience=10, model_dir=MODEL_DIR):\n",
    "    \"\"\"Train a model for a specific target\n",
    "    \n",
    "    Args:\n",
    "        data: Prepared data dictionary\n",
    "        target_col: Target column name\n",
    "        batch_size: Batch size\n",
    "        learning_rate: Learning rate\n",
    "        weight_decay: Weight decay for regularization\n",
    "        epochs: Number of epochs\n",
    "        patience: Patience for early stopping\n",
    "        model_dir: Directory to save models\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with training results\n",
    "    \"\"\"\n",
    "    # Get data\n",
    "    train_dataset = data[\"train_dataset\"]\n",
    "    val_dataset = data[\"val_dataset\"]\n",
    "    num_classes = data[\"num_classes\"]\n",
    "    text_input_dim = data[\"text_input_dim\"]\n",
    "    pose_input_dim = data[\"pose_input_dim\"]\n",
    "    class_weights = data[\"class_weights\"]\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    if val_dataset:\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    else:\n",
    "        val_loader = None\n",
    "    \n",
    "    # Initialize model\n",
    "    model = MultimodalExerciseClassifier(\n",
    "        text_input_dim=text_input_dim,\n",
    "        pose_input_dim=pose_input_dim,\n",
    "        num_classes=num_classes\n",
    "    ).to(device)\n",
    "    \n",
    "    # Define loss function with class weights\n",
    "    if class_weights is not None:\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Define optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    # Define learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Initialize variables for tracking\n",
    "    best_val_loss = float('inf')\n",
    "    early_stop_counter = 0\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    \n",
    "    # Training loop\n",
    "    print(f\"\\nTraining model for {target_col}...\")\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        # Create progress bar\n",
    "        train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} (Train)\")\n",
    "        \n",
    "        for text_features, pose_features, labels in train_bar:\n",
    "            # Move data to device\n",
    "            text_features = text_features.to(device)\n",
    "            pose_features = pose_features.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(text_features, pose_features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update statistics\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            train_bar.set_postfix({\n",
    "                \"loss\": loss.item(),\n",
    "                \"acc\": 100 * correct / total\n",
    "            })\n",
    "        \n",
    "        # Calculate epoch statistics\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        train_acc = 100 * correct / total\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        \n",
    "        # Validation\n",
    "        if val_loader:\n",
    "            val_loss, val_acc = validate(model, val_loader, criterion)\n",
    "            val_losses.append(val_loss)\n",
    "            val_accs.append(val_acc)\n",
    "            \n",
    "            # Print epoch results\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - \"\n",
    "                  f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% - \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "            \n",
    "            # Update learning rate scheduler\n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "            # Check for early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                early_stop_counter = 0\n",
    "                \n",
    "                # Save best model\n",
    "                os.makedirs(model_dir, exist_ok=True)\n",
    "                torch.save(model.state_dict(), f\"{model_dir}/{target_col}_model.pth\")\n",
    "                print(f\"  Saved best model with validation loss: {val_loss:.4f}\")\n",
    "            else:\n",
    "                early_stop_counter += 1\n",
    "                print(f\"  Early stopping counter: {early_stop_counter}/{patience}\")\n",
    "                \n",
    "                if early_stop_counter >= patience:\n",
    "                    print(f\"  Early stopping triggered at epoch {epoch+1}\")\n",
    "                    break\n",
    "        else:\n",
    "            # No validation set, just print training stats\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - \"\n",
    "                  f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "            \n",
    "            # Save model at regular intervals\n",
    "            if (epoch + 1) % 5 == 0 or epoch == epochs - 1:\n",
    "                os.makedirs(model_dir, exist_ok=True)\n",
    "                torch.save(model.state_dict(), f\"{model_dir}/{target_col}_model.pth\")\n",
    "                print(f\"  Saved model checkpoint at epoch {epoch+1}\")\n",
    "    \n",
    "    # Load best model if validation was used\n",
    "    if val_loader and os.path.exists(f\"{model_dir}/{target_col}_model.pth\"):\n",
    "        model.load_state_dict(torch.load(f\"{model_dir}/{target_col}_model.pth\"))\n",
    "        print(f\"Loaded best model from {model_dir}/{target_col}_model.pth\")\n",
    "    \n",
    "    # Plot training curves\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    if val_loader:\n",
    "        plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.title(f'{target_col} - Loss Curves')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{model_dir}/{target_col}_training_curves.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        \"target_col\": target_col,\n",
    "        \"num_classes\": num_classes,\n",
    "        \"text_input_dim\": text_input_dim,\n",
    "        \"pose_input_dim\": pose_input_dim,\n",
    "        \"epochs_trained\": len(train_losses),\n",
    "        \"best_val_loss\": best_val_loss if val_loader else None,\n",
    "        \"final_train_acc\": train_accs[-1],\n",
    "        \"final_val_acc\": val_accs[-1] if val_loader else None,\n",
    "        \"feature_names\": data[\"feature_names\"]\n",
    "    }\n",
    "    \n",
    "    # Save scalers\n",
    "    joblib.dump(data[\"text_scaler\"], f\"{model_dir}/{target_col}_text_scaler.pkl\")\n",
    "    joblib.dump(data[\"pose_scaler\"], f\"{model_dir}/{target_col}_pose_scaler.pkl\")\n",
    "    \n",
    "    # Save label encoder\n",
    "    joblib.dump(data[\"encoder\"], f\"{model_dir}/{target_col}_encoder.pkl\")\n",
    "    \n",
    "    # Save metadata\n",
    "    joblib.dump(metadata, f\"{model_dir}/{target_col}_metadata.pkl\")\n",
    "    \n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"train_losses\": train_losses,\n",
    "        \"train_accs\": train_accs,\n",
    "        \"val_losses\": val_losses if val_loader else None,\n",
    "        \"val_accs\": val_accs if val_loader else None,\n",
    "        \"metadata\": metadata\n",
    "    }\n",
    "\n",
    "def evaluate_model(model, dataset, encoder, target_col, batch_size=32, model_dir=MODEL_DIR):\n",
    "    \"\"\"Evaluate model on test data\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        dataset: Test dataset\n",
    "        encoder: Label encoder\n",
    "        target_col: Target column name\n",
    "        batch_size: Batch size\n",
    "        model_dir: Directory to save results\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with evaluation results\n",
    "    \"\"\"\n",
    "    # Create data loader\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize variables\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Evaluate model\n",
    "    with torch.no_grad():\n",
    "        for text_features, pose_features, labels in dataloader:\n",
    "            # Move data to device\n",
    "            text_features = text_features.to(device)\n",
    "            pose_features = pose_features.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(text_features, pose_features)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            # Store predictions and labels\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    # Get class names\n",
    "    class_names = encoder.classes_\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    # Generate classification report\n",
    "    report = classification_report(\n",
    "        all_labels, all_preds,\n",
    "        target_names=class_names,\n",
    "        output_dict=True\n",
    "    )\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(f\"Confusion Matrix - {target_col}\")\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{model_dir}/{target_col}_confusion_matrix.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Print classification report\n",
    "    print(f\"\\nEvaluation Results for {target_col}:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=class_names))\n",
    "    \n",
    "    # Return evaluation results\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"classification_report\": report,\n",
    "        \"confusion_matrix\": cm,\n",
    "        \"predictions\": all_preds,\n",
    "        \"true_labels\": all_labels,\n",
    "        \"class_names\": class_names\n",
    "    }\n",
    "\n",
    "def predict(model, text_features, pose_features, text_scaler, pose_scaler, encoder):\n",
    "    \"\"\"Make predictions with a trained model\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        text_features: Text features\n",
    "        pose_features: Pose features\n",
    "        text_scaler: Text feature scaler\n",
    "        pose_scaler: Pose feature scaler\n",
    "        encoder: Label encoder\n",
    "        \n",
    "    Returns:\n",
    "        Predicted labels\n",
    "    \"\"\"\n",
    "    # Scale features\n",
    "    text_scaled = text_scaler.transform(text_features)\n",
    "    pose_scaled = pose_scaler.transform(pose_features)\n",
    "    \n",
    "    # Convert to tensors\n",
    "    text_tensor = torch.tensor(text_scaled, dtype=torch.float32).to(device)\n",
    "    pose_tensor = torch.tensor(pose_scaled, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(text_tensor, pose_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    # Convert to numpy and decode\n",
    "    predictions = predicted.cpu().numpy()\n",
    "    decoded_predictions = encoder.inverse_transform(predictions)\n",
    "    \n",
    "    return decoded_predictions\n",
    "\n",
    "def load_model(target_col, model_dir=MODEL_DIR):\n",
    "    \"\"\"Load a trained model and associated data\n",
    "    \n",
    "    Args:\n",
    "        target_col: Target column name\n",
    "        model_dir: Directory containing saved models\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with loaded model and associated data\n",
    "    \"\"\"\n",
    "    # Check if model files exist\n",
    "    model_path = f\"{model_dir}/{target_col}_model.pth\"\n",
    "    metadata_path = f\"{model_dir}/{target_col}_metadata.pkl\"\n",
    "    text_scaler_path = f\"{model_dir}/{target_col}_text_scaler.pkl\"\n",
    "    pose_scaler_path = f\"{model_dir}/{target_col}_pose_scaler.pkl\"\n",
    "    encoder_path = f\"{model_dir}/{target_col}_encoder.pkl\"\n",
    "    \n",
    "    if not os.path.exists(model_path) or not os.path.exists(metadata_path):\n",
    "        print(f\"Model for {target_col} not found\")\n",
    "        return None\n",
    "    \n",
    "    # Load metadata\n",
    "    metadata = joblib.load(metadata_path)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = MultimodalExerciseClassifier(\n",
    "        text_input_dim=metadata[\"text_input_dim\"],\n",
    "        pose_input_dim=metadata[\"pose_input_dim\"],\n",
    "        num_classes=metadata[\"num_classes\"]\n",
    "    )\n",
    "    \n",
    "    # Load model weights\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Load scalers and encoder\n",
    "    text_scaler = joblib.load(text_scaler_path) if os.path.exists(text_scaler_path) else None\n",
    "    pose_scaler = joblib.load(pose_scaler_path) if os.path.exists(pose_scaler_path) else None\n",
    "    encoder = joblib.load(encoder_path) if os.path.exists(encoder_path) else None\n",
    "    \n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"metadata\": metadata,\n",
    "        \"text_scaler\": text_scaler,\n",
    "        \"pose_scaler\": pose_scaler,\n",
    "        \"encoder\": encoder\n",
    "    }\n",
    "\n",
    "def validate(model, dataloader, criterion):\n",
    "    \"\"\"Validate model on a dataloader\n",
    "    \n",
    "    Args:\n",
    "        model: Model to validate\n",
    "        dataloader: Validation dataloader\n",
    "        criterion: Loss function\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (validation loss, validation accuracy)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text_features, pose_features, labels in dataloader:\n",
    "            # Move data to device\n",
    "            text_features = text_features.to(device)\n",
    "            pose_features = pose_features.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(text_features, pose_features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Update statistics\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    # Calculate validation metrics\n",
    "    val_loss = val_loss / len(dataloader)\n",
    "    val_acc = 100 * correct / total\n",
    "    \n",
    "    return val_loss, val_acc\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accs, label='Train Accuracy')\n",
    "    if val_loader:\n",
    "        plt.plot(val_accs, label='Validation Accuracy')\n",
    "    plt.title(f'{target_col} - Accuracy Curves')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    \n
        return val_loss, val_acc\n",
    "\n",
    "\n",
    "## 5. Train Models for Each Target Variable\n",
    "\n",
    "Now let's train models for each of our target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Select target variables to train models for\n",
    "if features_df is not None and 'feature_columns' in locals():\n",
    "    # You can select specific targets here or use all available ones\n",
    "    target_variables = feature_columns['targets']\n",
    "    \n",
    "    print(f\"Training models for {len(target_variables)} target variables:\")\n",
    "    for target in target_variables:\n",
    "        print(f\"  - {target}\")\n",
    "    \n",
    "    # Ask for confirmation\n",
    "    confirm = input(\"\\nProceed with training? (y/n): \")\n",
    "    \n",
    "    if confirm.lower() == 'y':\n",
    "        # Dictionary to store training results\n",
    "        training_results = {}\n",
    "        \n",
    "        # Train a model for each target variable\n",
    "        for target in target_variables:\n",
    "            print(f\"\\n{'='*80}\\nTraining model for {target}\\n{'='*80}\")\n",
    "            \n",
    "            # Prepare data for this target\n",
    "            data = prepare_data(\n",
    "                features_df, \n",
    "                target, \n",
    "                feature_columns['text'], \n",
    "                feature_columns['pose'],\n",
    "                test_size=0.2,\n",
    "                val_size=0.1\n",
    "            )\n",
    "            \n",
    "            # Train model\n",
    "            result = train_model(\n",
    "                data,\n",
    "                target,\n",
    "                batch_size=32,\n",
    "                learning_rate=0.001,\n",
    "                epochs=30,\n",
    "                patience=5\n",
    "            )\n",
    "            \n",
    "            # Evaluate model\n",
    "            print(f\"\\nEvaluating model for {target}\")\n",
    "            eval_result = evaluate_model(\n",
    "                result['model'],\n",
    "                data['test_dataset'],\n",
    "                data['encoder'],\n",
    "                target\n",
    "            )\n",
    "            \n",
    "            # Store results\n",
    "            training_results[target] = {\n",
    "                \"training\": result,\n",
    "                \"evaluation\": eval_result\n",
    "            }\n",
    "        \n",
    "        print(\"\\nTraining complete! Results summary:\")\n",
    "        for target, result in training_results.items():\n",
    "            print(f\"\\n{target}:\")\n",
    "            print(f\"  Accuracy: {result['evaluation']['accuracy']:.4f}\")\n",
    "            if 'classification_report' in result['evaluation']:\n",
    "                report = result['evaluation']['classification_report']\n",
    "                if 'weighted avg' in report:\n",
    "                    print(f\"  Precision: {report['weighted avg']['precision']:.4f}\")\n",
    "                    print(f\"  Recall: {report['weighted avg']['recall']:.4f}\")\n",
    "                    print(f\"  F1 Score: {report['weighted avg']['f1-score']:.4f}\")\n",
    "    else:\n",
    "        print(\"Training cancelled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Model Results\n",
    "\n",
    "Let's visualize the results of our trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize model results\n",
    "if 'training_results' in locals() and training_results:\n",
    "    # Create figure for accuracy comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Extract accuracies\n",
    "    targets = list(training_results.keys())\n",
    "    accuracies = [result['evaluation']['accuracy'] for result in training_results.values()]\n",
    "    \n",
    "    # Sort by accuracy\n",
    "    sorted_indices = np.argsort(accuracies)\n",
    "    sorted_targets = [targets[i] for i in sorted_indices]\n",
    "    sorted_accuracies = [accuracies[i] for i in sorted_indices]\n",
    "    \n",
    "    # Plot accuracies\n",
    "    ax = sns.barplot(x=sorted_accuracies, y=sorted_targets)\n",
    "    \n",
    "    # Add accuracy values\n",
    "    for i, acc in enumerate(sorted_accuracies):\n",
    "        ax.text(acc + 0.02, i, f\"{acc:.4f}\", va='center')\n",
    "    \n",
    "    plt.title(\"Model Accuracy Comparison\")\n",
    "    plt.xlabel(\"Accuracy\")\n",
    "    plt.ylabel(\"Target Variable\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{RESULTS_DIR}/model_accuracy_comparison.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Make Predictions on New Data\n",
    "\n",
    "Let's show how to use the trained models to make predictions on new exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load a trained model and make predictions\n",
    "if 'feature_columns' in locals() and 'feature_columns' in locals():\n",
    "    # Select a target to make predictions for\n",
    "    target_to_predict = feature_columns['targets'][0]  # Use first target as example\n",
    "    \n",
    "    print(f\"Loading model for {target_to_predict}\")\n",
    "    loaded_model = load_model(target_to_predict)\n",
    "    \n",
    "    if loaded_model:\n",
    "        print(f\"Model loaded successfully\")\n",
    "        print(f\"Model metadata: {loaded_model['metadata']}\")\n",
    "        \n",
    "        # Select some examples to predict\n",
    "        num_examples = 5\n",
    "        examples = features_df.sample(num_examples)\n",
    "        \n",
    "        # Extract features\n",
    "        text_features = examples[feature_columns['text']].fillna(0).values if feature_columns['text'] else np.zeros((num_examples, 1))\n",
    "        pose_features = examples[feature_columns['pose']].fillna(0).values if feature_columns['pose'] else np.zeros((num_examples, 1))\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = predict(\n",
    "            loaded_model['model'],\n",
    "            text_features,\n",
    "            pose_features,\n",
    "            loaded_model['text_scaler'],\n",
    "            loaded_model['pose_scaler'],\n",
    "            loaded_model['encoder']\n",
    "        )\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\nPredictions for {target_to_predict}:\")\n",
    "        for i, (_, example) in enumerate(examples.iterrows()):\n",
    "            print(f\"\\nExample {i+1}:\")\n",
    "            print(f\"  Name: {example.get('name', 'Unknown')}\")\n",
    "            print(f\"  Body Part: {example.get('bodypart', 'Unknown')}\")\n",
    "            print(f\"  Actual {target_to_predict}: {example.get(target_to_predict, 'Unknown')}\")\n",
    "            print(f\"  Predicted {target_to_predict}: {predictions[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create a Prediction Pipeline for All Models\n",
    "\n",
    "Finally, let's create a function to make predictions using all trained models at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def predict_all_attributes(example_data, feature_columns, model_dir=MODEL_DIR):\n",
    "    \"\"\"Predict all attributes for an exercise\n",
    "    \n",
    "    Args:\n",
    "        example_data: DataFrame with exercise data\n",
    "        feature_columns: Dictionary of feature column lists\n",
    "        model_dir: Directory containing saved models\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of predictions for each target variable\n",
    "    \"\"\"\n",
    "    # Check if input is a single row or DataFrame\n",
    "    if len(example_data) == 0:\n",
    "        print(\"No examples provided\")\n",
    "        return {}\n",
    "    \n",
    "    # Find available models\n",
    "    available_targets = []\n",
    "    for target in feature_columns['targets']:\n",
    "        if os.path.exists(f\"{model_dir}/{target}_model.pth\"):\n",
    "            available_targets.append(target)\n",
    "    \n",
    "    if not available_targets:\n",
    "        print(\"No trained models found\")\n",
    "        return {}\n",
    "    \n",
    "    print(f\"Found {len(available_targets)} trained models: {available_targets}\")\n",
    "    \n",
    "    # Dictionary to store predictions\n",
    "    all_predictions = {}\n",
    "    \n",
    "    # Make predictions for each target\n",
    "    for target in available_targets:\n",
    "        # Load model\n",
    "        loaded_model = load_model(target, model_dir)\n",
    "        \n",
    "        if loaded_model:\n",
    "            # Extract features\n",
    "            text_feature_cols = loaded_model['metadata']['feature_names']['text']\n",
    "            pose_feature_cols = loaded_model['metadata']['feature_names']['pose']\n",
    "            \n",
    "            # Check which features are available in the input data\n",
    "            available_text_cols = [col for col in text_feature_cols if col in example_data.columns]\n",
    "            available_pose_cols = [col for col in pose_feature_cols if col in example_data.columns]\n",
    "            \n",
    "            # If some features are missing, print a warning\n",
    "            if len(available_text_cols) < len(text_feature_cols):\n",
    "                print(f\"Warning: {len(text_feature_cols) - len(available_text_cols)} text features missing for {target}\")\n",
    "            \n",
    "            if len(available_pose_cols) < len(pose_feature_cols):\n",
    "                print(f\"Warning: {len(pose_feature_cols) - len(available_pose_cols)} pose features missing for {target}\")\n",
    "            \n",
    "            # If no features are available, skip this target\n",
    "            if not available_text_cols and not available_pose_cols:\n",
    "                print(f\"Skipping {target} due to missing features\")\n",
    "                continue\n",
    "            \n",
    "            # Extract available features, using zeros for missing ones\n",
    "            if available_text_cols:\n",
    "                text_features = example_data[available_text_cols].fillna(0).values\n",
    "                # If some features are missing, pad with zeros\n",
    "                if len(available_text_cols) < len(text_feature_cols):\n",
    "                    padding = np.zeros((len(example_data), len(text_feature_cols) - len(available_text_cols)))\n",
    "                    text_features = np.hstack((text_features, padding))\n",
    "            else:\n",
    "                text_features = np.zeros((len(example_data), loaded_model['metadata']['text_input_dim']))\n",
    "            \n",
    "            if available_pose_cols:\n",
    "                pose_features = example_data[available_pose_cols].fillna(0).values\n",
    "                # If some features are missing, pad with zeros\n",
    "                if len(available_pose_cols) < len(pose_feature_cols):\n",
    "                    padding = np.zeros((len(example_data), len(pose_feature_cols) - len(available_pose_cols)))\n",
    "                    pose_features = np.hstack((pose_features, padding))\n",
    "            else:\n",
    "                pose_features = np.zeros((len(example_data), loaded_model['metadata']['pose_input_dim']))\n",
    "            \n",
    "            # Make predictions\n",
    "            predictions = predict(\n",
    "                loaded_model['model'],\n",
    "                text_features,\n",
    "                pose_features,\n",
    "                loaded_model['text_scaler'],\n",
    "                loaded_model['pose_scaler'],\n",
    "                loaded_model['encoder']\n",
    "            )\n",
    "            \n",
    "            # Store predictions\n",
    "            all_predictions[target] = predictions\n",
    "    \n",
    "    return all_predictions\n",
    "\n",
    "# Test the prediction pipeline on a few examples\n",
    "if features_df is not None and 'feature_columns' in locals():\n",
    "    # Select a few examples\n",
    "    num_examples = 3\n",
    "    examples = features_df.sample(num_examples)\n",
    "    \n",
    "    # Make predictions\n",
    "    print(f\"Making predictions for {num_examples} example exercises...\")\n",
    "    predictions = predict_all_attributes(examples, feature_columns)\n",
    "    \n",
    "    if predictions:\n",
    "        # Create a DataFrame to compare actual vs. predicted values\n",
    "        comparison_df = pd.DataFrame()\n",
    "        \n",
    "        # Add basic info\n",
    "        if 'name' in examples.columns:\n",
    "            comparison_df['name'] = examples['name']\n",
    "        if 'bodypart' in examples.columns:\n",
    "            comparison_df['bodypart'] = examples['bodypart']\n",
    "        \n",
    "        # Add actual and predicted values\n",
    "        for target in predictions.keys():\n",
    "            if target in examples.columns:\n",
    "                comparison_df[f\"{target}_actual\"] = examples[target]\n",
    "            comparison_df[f\"{target}_predicted\"] = predictions[target]\n",
    "        \n",
    "        # Display comparison\n",
    "        print(\"\\nPrediction Results:\")\n",
    "        display(comparison_df)\n",
    "\n",
    "\n",
    "## 9. Model Deployment Considerations\n",
    "\n",
    "In a production environment, you would need to create an API or service to use these models. Here's a simple example of how to structure a prediction function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def predict_exercise_attributes(exercise_data, model_dir=MODEL_DIR):\n",
    "    \"\"\"Predict attributes for a new exercise\n",
    "    \n",
    "    Args:\n",
    "        exercise_data: Dictionary or JSON with exercise data\n",
    "            Should include name, description, and possibly image URL\n",
    "        model_dir: Directory containing saved models\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with predicted attributes\n",
    "    \"\"\"\n",
    "    # This is a placeholder function that outlines how you would\n",
    "    # implement this in a production system\n",
    "    \n",
    "    # 1. Extract text features from name and description\n",
    "    # You would use the saved text feature extractor here\n",
    "    \n",
    "    # 2. Download and process image to extract pose features\n",
    "    # You would use the saved pose feature extractor here\n",
    "    \n",
    "    # 3. Load all available models\n",
    "    available_targets = []\n",
    "    for file in os.listdir(model_dir):\n",
    "        if file.endswith(\"_model.pth\"):\n",
    "            target = file.split(\"_model.pth\")[0]\n",
    "            available_targets.append(target)\n",
    "    \n",
    "    # 4. Make predictions with each model\n",
    "    predictions = {}\n",
    "    for target in available_targets:\n",
    "        # Load model and associated data\n",
    "        loaded_model = load_model(target, model_dir)\n",
    "        \n",
    "        if loaded_model:\n",
    "            # Make prediction\n",
    "            # This would be implemented using the actual features\n",
    "            predictions[target] = \"Placeholder prediction\"\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "\n",
    "# Example of how this would be used in an API endpoint\n",
    "def api_endpoint_example():\n",
    "    \"\"\"Example of how to use the prediction function in an API endpoint\"\"\"\n",
    "    # This is just a demonstration and won't actually run\n",
    "    \n",
    "    # In a Flask API, you might have something like:\n",
    "    \"\"\"\n",
    "    @app.route('/predict', methods=['POST'])\n",
    "    def predict():\n",
    "        # Get data from request\n",
    "        exercise_data = request.json\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = predict_exercise_attributes(exercise_data)\n",
    "        \n",
    "        # Return predictions\n",
    "        return jsonify(predictions)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sample input data\n",
    "    sample_input = {\n",
    "        \"name\": \"Barbell Squat\",\n",
    "        \"description\": \"Stand with feet shoulder-width apart, barbell across upper back. Bend knees and lower body until thighs are parallel to floor. Return to starting position.\",\n",
    "        \"image_url\": \"https://example.com/barbell_squat.jpg\"\n",
    "    }\n",
    "    \n",
    "    # Sample output\n",
    "    sample_output = {\n",
    "        \"movement_pattern\": \"squat\",\n",
    "        \"is_compound\": True,\n",
    "        \"exercise_type\": \"strength\",\n",
    "        \"intensity_level\": \"intermediate\",\n",
    "        \"risk_assessment\": \"medium\",\n",
    "        \"movement_quality\": 4.2\n",
    "    }\n",
    "    \n",
    "    return sample_input, sample_output\n",
    "\n",
    "\n",
    "# Show sample API input and output\n",
    "sample_input, sample_output = api_endpoint_example()\n",
    "print(\"Sample API Input:\")\n",
    "print(json.dumps(sample_input, indent=2))\n",
    "print(\"\\nSample API Output:\")\n",
    "print(json.dumps(sample_output, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to train and use deep learning models for exercise classification. Here's a summary of what we've covered:\n",
    "\n",
    "1. **Data Preparation**:\n",
    "   - Loaded processed features from the feature engineering phase\n",
    "   - Split data into training, validation, and test sets\n",
    "   - Handled class imbalance with weighted loss functions\n",
    "\n",
    "2. **Model Architecture**:\n",
    "   - Created a multimodal neural network that combines text and pose features\n",
    "   - Used separate processing branches for each feature type\n",
    "   - Combined features for final classification\n",
    "\n",
    "3. **Model Training**:\n",
    "   - Trained models for multiple target variables (movement patterns, exercise types, etc.)\n",
    "   - Implemented early stopping and learning rate scheduling\n",
    "   - Saved model weights and associated data for later use\n",
    "\n",
    "4. **Model Evaluation**:\n",
    "   - Evaluated models on held-out test data\n",
    "   - Generated confusion matrices and classification reports\n",
    "   - Compared performance across different target variables\n",
    "\n",
    "5. **Prediction Pipeline**:\n",
    "   - Created functions to load trained models and make predictions\n",
    "   - Built a pipeline to predict all attributes at once\n",
    "   - Demonstrated how to handle missing features\n",
    "\n",
    "6. **Deployment Considerations**:\n",
    "   - Outlined how to use these models in a production environment\n",
    "   - Provided a sample API endpoint structure\n",
    "\n",
    "The models trained in this notebook can be used to automatically classify exercises based on their descriptions and images, providing valuable information for fitness applications, workout planning, and exercise recommendation systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
      
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}