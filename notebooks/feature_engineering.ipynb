{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Classification Feature Engineering\n",
    "\n",
    "This notebook demonstrates the feature engineering process for the exercise classification project. We'll extract features from exercise data to classify exercises based on various attributes including:\n",
    "\n",
    "- Target muscle groups\n",
    "- Required equipment\n",
    "- Movement patterns\n",
    "- Intensity/experience levels\n",
    "- Exercise types\n",
    "- Quality of movement\n",
    "- Risk assessment\n",
    "\n",
    "We'll use both text-based features from exercise descriptions and pose-based features from exercise images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set Matplotlib and Seaborn styles\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set_palette('Set2')\n",
    "\n",
    "# For displaying images in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Raw Exercise Data\n",
    "\n",
    "First, let's load the raw exercise data that we fetched from the APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExerciseDB data not found. Please run the data fetcher first.\n"
     ]
    }
   ],
   "source": [
    "# Set data paths\n",
    "RAW_DATA_DIR = \"data/raw\"\n",
    "PROCESSED_DATA_DIR = \"data/processed\"\n",
    "EXERCISEDB_DIR = f\"{RAW_DATA_DIR}/exercisedb\"\n",
    "MODEL_DIR = \"models\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Load ExerciseDB data\n",
    "exercisedb_path = f\"{EXERCISEDB_DIR}/all_exercises.csv\"\n",
    "\n",
    "if os.path.exists(exercisedb_path):\n",
    "    df = pd.read_csv(exercisedb_path)\n",
    "    print(f\"Loaded {len(df)} exercises from ExerciseDB\")\n",
    "else:\n",
    "    print(\"ExerciseDB data not found. Please run the data fetcher first.\")\n",
    "    df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic info and sample data\n",
    "if df is not None:\n",
    "    print(\"\\nDataFrame Info:\")\n",
    "    df.info()\n",
    "    \n",
    "    print(\"\\nSample Data:\")\n",
    "    display(df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initial Data Preprocessing\n",
    "\n",
    "Let's clean and prepare the data for feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(df):\n",
    "    \"\"\"Clean and preprocess the raw dataset\"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    cleaned_df = df.copy()\n",
    "    \n",
    "    # Make column names lowercase and replace spaces with underscores\n",
    "    cleaned_df.columns = [col.lower().replace(' ', '_') for col in cleaned_df.columns]\n",
    "    \n",
    "    # Convert all string columns to lowercase\n",
    "    for col in cleaned_df.select_dtypes(include=['object']).columns:\n",
    "        if col in ['instructions', 'name', 'bodypart', 'target', 'equipment']:\n",
    "            cleaned_df[col] = cleaned_df[col].str.lower()\n",
    "    \n",
    "    # Fill missing values\n",
    "    if 'instructions' in cleaned_df.columns:\n",
    "        cleaned_df['instructions'].fillna('', inplace=True)\n",
    "    \n",
    "    return cleaned_df\n",
    "\n",
    "# Clean the dataset\n",
    "if df is not None:\n",
    "    cleaned_df = clean_dataset(df)\n",
    "    print(f\"Cleaned dataset with {len(cleaned_df)} rows and {len(cleaned_df.columns)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the unique values in key categorical columns\n",
    "if 'cleaned_df' in locals():\n",
    "    categorical_cols = ['bodypart', 'target', 'equipment']\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if col in cleaned_df.columns:\n",
    "            unique_vals = cleaned_df[col].unique()\n",
    "            print(f\"\\n{col.capitalize()} unique values ({len(unique_vals)}):\")\n",
    "            print(sorted(unique_vals)[:10], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Feature Extraction\n",
    "\n",
    "Now, let's extract features from the text data (exercise names and instructions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextFeatureExtractor:\n",
    "    \"\"\"Extract features from exercise text data\"\"\"\n",
    "    def __init__(self, max_features=100):\n",
    "        self.max_features = max_features\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(\n",
    "            max_features=max_features,\n",
    "            stop_words='english',\n",
    "            ngram_range=(1, 2),\n",
    "            min_df=2\n",
    "        )\n",
    "    \n",
    "    def extract_features(self, df):\n",
    "        \"\"\"Extract text features from a DataFrame\"\"\"\n",
    "        # Combine name and instructions for TF-IDF\n",
    "        if 'instructions' in df.columns and 'name' in df.columns:\n",
    "            df['text_combined'] = df['name'] + ' ' + df['instructions'].fillna('')\n",
    "        elif 'name' in df.columns:\n",
    "            df['text_combined'] = df['name']\n",
    "        else:\n",
    "            raise ValueError(\"DataFrame must contain 'name' column\")\n",
    "        \n",
    "        # Extract TF-IDF features\n",
    "        print(\"Extracting TF-IDF features...\")\n",
    "        tfidf_matrix = self.tfidf_vectorizer.fit_transform(df['text_combined'])\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        feature_names = self.tfidf_vectorizer.get_feature_names_out()\n",
    "        tfidf_df = pd.DataFrame(\n",
    "            tfidf_matrix.toarray(), \n",
    "            columns=[f\"tfidf_{f}\" for f in feature_names]\n",
    "        )\n",
    "        \n",
    "        return pd.concat([df, tfidf_df], axis=1)\n",
    "\n",
    "# Initialize and use the text feature extractor\n",
    "if 'cleaned_df' in locals():\n",
    "    text_extractor = TextFeatureExtractor(max_features=100)\n",
    "    text_features_df = text_extractor.extract_features(cleaned_df)\n",
    "    \n",
    "    # Display text features\n",
    "    text_feature_cols = [col for col in text_features_df.columns if col.startswith('tfidf_')]\n",
    "    print(f\"\\nExtracted {len(text_feature_cols)} text features\")\n",
    "    \n",
    "    # Save intermediate result\n",
    "    text_features_df.to_csv(f\"{PROCESSED_DATA_DIR}/text_features.csv\", index=False)\n",
    "    print(f\"Text features saved to {PROCESSED_DATA_DIR}/text_features.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pose Feature Extraction\n",
    "\n",
    "Now, let's extract features from exercise images using MediaPipe pose estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseFeatureExtractor:\n",
    "    \"\"\"Extract features from exercise images using MediaPipe pose estimation\"\"\"\n",
    "    def __init__(self, confidence_threshold=0.5):\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        self.mp_pose = mp.solutions.pose\n",
    "        self.pose = self.mp_pose.Pose(\n",
    "            static_image_mode=True,\n",
    "            model_complexity=2,\n",
    "            min_detection_confidence=confidence_threshold\n",
    "        )\n",
    "    \n",
    "    def download_and_process_images(self, df, limit=None):\n",
    "        \"\"\"Download and process exercise images\"\"\"\n",
    "        if 'gifUrl' not in df.columns:\n",
    "            raise ValueError(\"DataFrame must contain 'gifUrl' column\")\n",
    "        \n",
    "        # Limit the number of images to process\n",
    "        process_df = df.head(limit) if limit else df\n",
    "        \n",
    "        # Process images and extract pose landmarks\n",
    "        landmarks_list = []\n",
    "        \n",
    "        for i, row in tqdm(process_df.iterrows(), total=len(process_df), desc=\"Processing images\"):\n",
    "            try:\n",
    "                # Download image\n",
    "                response = requests.get(row['gifUrl'])\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                # For GIFs, extract the first frame\n",
    "                image = Image.open(BytesIO(response.content))\n",
    "                if hasattr(image, 'n_frames') and image.n_frames > 1:\n",
    "                    image.seek(0)\n",
    "                \n",
    "                # Convert to numpy array for OpenCV\n",
    "                image_np = np.array(image.convert('RGB'))\n",
    "                \n",
    "                # Process with MediaPipe\n",
    "                results = self.pose.process(image_np)\n",
    "                \n",
    "                if results.pose_landmarks:\n",
    "                    # Extract landmark positions\n",
    "                    landmarks = []\n",
    "                    for landmark in results.pose_landmarks.landmark:\n",
    "                        landmarks.extend([landmark.x, landmark.y, landmark.z, landmark.visibility])\n",
    "                else:\n",
    "                    # If no landmarks detected, fill with zeros\n",
    "                    landmarks = [0] * (33 * 4)  # 33 landmarks with x,y,z,visibility\n",
    "                \n",
    "                landmarks_list.append(landmarks)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image {row['gifUrl']}: {e}\")\n",
    "                landmarks_list.append([0] * (33 * 4))\n",
    "        \n",
    "        # Create feature columns for landmarks\n",
    "        landmark_columns = []\n",
    "        for i in range(33):\n",
    "            for dim in ['x', 'y', 'z', 'vis']:\n",
    "                landmark_columns.append(f\"landmark_{i}_{dim}\")\n",
    "        \n",
    "        landmarks_df = pd.DataFrame(landmarks_list, columns=landmark_columns)\n",
    "        \n",
    "        # Add landmark columns to original DataFrame\n",
    "        result_df = process_df.copy()\n",
    "        for col in landmark_columns:\n",
    "            result_df[col] = landmarks_df[col]\n",
    "        \n",
    "        return result_df\n",
    "\n",
    "# Initialize and use the pose feature extractor\n",
    "if 'cleaned_df' in locals():\n",
    "    pose_extractor = PoseFeatureExtractor(confidence_threshold=0.5)\n",
    "    pose_features_df = pose_extractor.download_and_process_images(cleaned_df, limit=100)  # Limit for testing\n",
    "    \n",
    "    # Display pose features\n",
    "    pose_feature_cols = [col for col in pose_features_df.columns if col.startswith('landmark_')]\n",
    "    print(f\"\\nExtracted {len(pose_feature_cols)} pose features\")\n",
    "    \n",
    "    # Save intermediate result\n",
    "    pose_features_df.to_csv(f\"{PROCESSED_DATA_DIR}/pose_features.csv\", index=False)\n",
    "    print(f\"Pose features saved to {PROCESSED_DATA_DIR}/pose_features.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Analysis\n",
    "\n",
    "Let's analyze the extracted features to understand their distribution and importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze text features\n",
    "if 'text_features_df' in locals():\n",
    "    # Display top TF-IDF terms\n",
    "    print(\"Top TF-IDF Terms:\")\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'term': text_extractor.tfidf_vectorizer.get_feature_names_out(),\n",
    "        'importance': text_features_df[text_feature_cols].mean()\n",
    "    })\n",
    "    display(feature_importance.sort_values('importance', ascending=False).head(10))\n",
    "    \n",
    "    # Visualize feature distribution\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    text_features_df[text_feature_cols[:10]].boxplot()\n",
    "    plt.title(\"Distribution of Text Features\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Analyze pose features\n",
    "if 'pose_features_df' in locals():\n",
    "    # Visualize landmark distribution\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    pose_features_df[pose_feature_cols[:10]].boxplot()\n",
    "    plt.title(\"Distribution of Pose Features\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Combination\n",
    "\n",
    "Finally, let's combine the text and pose features into a unified feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedFeatureExtractor:\n",
    "    \"\"\"Combine text and pose features into a unified feature set\"\"\"\n",
    "    def __init__(self, text_extractor=None, pose_extractor=None, use_pca=True, pca_components=50):\n",
    "        self.text_extractor = text_extractor or TextFeatureExtractor()\n",
    "        self.pose_extractor = pose_extractor or PoseFeatureExtractor()\n",
    "        self.use_pca = use_pca\n",
    "        self.pca_components = pca_components\n",
    "        \n",
    "        self.text_scaler = StandardScaler()\n",
    "        self.pose_scaler = StandardScaler()\n",
    "        self.combined_scaler = StandardScaler()\n",
    "        \n",
    "        if use_pca:\n",
    "            self.text_pca = PCA(n_components=pca_components)\n",
    "            self.pose_pca = PCA(n_components=pca_components)\n",
    "    \n",
    "    def extract_all_features(self, df, process_images=True, image_limit=None):\n",
    "        \"\"\"Extract and combine all features\"\"\"\n",
    "        # Extract text features\n",
    "        text_features_df = self.text_extractor.extract_features(df)\n",
    "        \n",
    "        # Extract pose features if requested\n",
    "        if process_images:\n",
    "            pose_features_df = self.pose_extractor.download_and_process_images(df, limit=image_limit)\n",
    "        else:\n",
    "            pose_features_df = df.copy()\n",
    "        \n",
    "        # Combine features\n",
    "        text_feature_cols = [col for col in text_features_df.columns if col.startswith('tfidf_')]\n",
    "        pose_feature_cols = [col for col in pose_features_df.columns if col.startswith('landmark_')]\n",
    "        \n",
    "        # Scale features\n",
    "        if text_feature_cols:\n",
    "            text_features = text_features_df[text_feature_cols].values\n",
    "            text_features_scaled = self.text_scaler.fit_transform(text_features)\n",
    "            \n",
    "            if self.use_pca:\n",
    "                text_features_pca = self.text_pca.fit_transform(text_features_scaled)\n",
    "                for i in range(text_features_pca.shape[1]):\n",
    "                    text_features_df[f'text_pca_{i}'] = text_features_pca[:, i]\n",
    "        \n",
    "        if pose_feature_cols:\n",
    "            pose_features = pose_features_df[pose_feature_cols].values\n",
    "            pose_features_scaled = self.pose_scaler.fit_transform(pose_features)\n",
    "            \n",
    "            if self.use_pca:\n",
    "                pose_features_pca = self.pose_pca.fit_transform(pose_features_scaled)\n",
    "                for i in range(pose_features_pca.shape[1]):\n",
    "                    pose_features_df[f'pose_pca_{i}'] = pose_features_pca[:, i]\n",
    "        \n",
    "        # Merge DataFrames\n",
    "        result_df = pd.concat([text_features_df, pose_features_df], axis=1)\n",
    "        \n",
    "        return result_df\n",
    "\n",
    "# Initialize and use the combined feature extractor\n",
    "if 'cleaned_df' in locals():\n",
    "    combined_extractor = CombinedFeatureExtractor(\n",
    "        text_extractor=text_extractor,\n",
    "        pose_extractor=pose_extractor,\n",
    "        use_pca=True,\n",
    "        pca_components=50\n",
    "    )\n",
    "    \n",
    "    combined_features_df = combined_extractor.extract_all_features(\n",
    "        cleaned_df,\n",
    "        process_images=True,\n",
    "        image_limit=100  # Limit for testing\n",
    "    )\n",
    "    \n",
    "    # Display combined features\n",
    "    print(\"\\nCombined Features Shape:\", combined_features_df.shape)\n",
    "    print(\"\\nCombined Feature Columns:\")\n",
    "    combined_feature_cols = [col for col in combined_features_df.columns if col.startswith(('text_pca_', 'pose_pca_'))]\n",
    "    for col in combined_feature_cols:\n",
    "        print(f\"- {col}\")\n",
    "    \n",
    "    # Save final features\n",
    "    combined_features_df.to_csv(f\"{PROCESSED_DATA_DIR}/combined_features.csv\", index=False)\n",
    "    print(f\"\\nCombined features saved to {PROCESSED_DATA_DIR}/combined_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
